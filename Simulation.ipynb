{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9d5a9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "add665fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "16799be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_bernouli(theta = .5, n_samples = 1):\n",
    "    \"\"\"\n",
    "    Generating samples from Bernouli density funtion\n",
    "    \"\"\"\n",
    "    return (np.random.rand(n_samples) <= theta).astype(int)\n",
    "\n",
    "\n",
    "def task_act_stim(n_trials = 42, act = True, prop = .9):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ----------\n",
    "    \n",
    "    n_trials: int\n",
    "        The number of observations    \n",
    "    act : bool\n",
    "        True is Action Value learning and False is Stimulus Value Learning\n",
    "    prop: float\n",
    "        The probability of correcting pushed response against to pulled response \n",
    "        (Yellow response against to Blue response)\n",
    "        Balue is between 0 and 1\n",
    "        \n",
    "    Output\n",
    "    -------\n",
    "\n",
    "    data : pandas.DataFrame\n",
    "        Columns contains: 'trialNumber',\n",
    "                          'stimActFirst',\n",
    "                          'yellowOnLeftSide', \n",
    "                          'leftCanBePushed', \n",
    "                          'pushCorrect', \n",
    "                          'yellowCorrect'\n",
    "\n",
    "    \"\"\"    \n",
    "    \n",
    "    data = pd.DataFrame(columns=['Session',\n",
    "                                 'run',\n",
    "                                 'stimActFirst',\n",
    "                                 'block',\n",
    "                                 'trialNumber', \n",
    "                                 'stimActFirst', \n",
    "                                 'yellowOnLeftSide', \n",
    "                                 'leftCanBePushed',\n",
    "                                 'winAmtLeft',\n",
    "                                 'winAmtRight',\n",
    "                                 'pushCorrect', \n",
    "                                 'yellowCorrect'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    trialNumber = np.arange(1,  n_trials + 1)\n",
    "    \n",
    "    if(act):\n",
    "        stimActFirst = 'Act'\n",
    "    else:      \n",
    "        stimActFirst = 'Stim'\n",
    "        \n",
    "        \n",
    "    yellowOnLeftSide = sample_bernouli(theta = .5, n_samples = n_trials)\n",
    "    leftCanBePushed = sample_bernouli(theta = .5, n_samples = n_trials)  \n",
    "    pushCorrect = sample_bernouli(theta = prop, n_samples = n_trials)\n",
    "    yellowCorrect = sample_bernouli(theta = .5, n_samples = n_trials)\n",
    "   \n",
    "                \n",
    "    \n",
    "    data.trialNumber = trialNumber\n",
    "    data.stimActFirst = stimActFirst\n",
    "    data.yellowOnLeftSide = yellowOnLeftSide\n",
    "    data.leftCanBePushed = leftCanBePushed\n",
    "    data.pushCorrect = pushCorrect\n",
    "    data.yellowCorrect = yellowCorrect\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e6e642fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:64\u001b[0;36m\u001b[0m\n\u001b[0;31m    ,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def simulate_rl(task_design, alpha_A, alpha_C, weight, bet, n_trilas = 10, init_probability=.5):\n",
    "    \"\"\"\n",
    "    General Comment   \n",
    "    ----------\n",
    "    \n",
    "    Simulates a individual behavior for Action and Stimulus Value Learning \n",
    "    according to a RL model with the weightening parameter,\n",
    "\n",
    "    Notw that in this simulation, a simple Rescorla-Wagner rule is used for reinforcement learning\n",
    "    and the softmax function is used for the choice response\n",
    "\n",
    "    This function is to simulate data for, for example, parameter recovery.\n",
    "    Simulates data for one participant.\n",
    "    \n",
    "    Two rewarded feedback and non-rewarded feedback are presented in each trial.\n",
    "  \n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "\n",
    "    task_frame : pandas.DataFrame\n",
    "         Size of n_trials rows.\n",
    "         Columns are related to Action and Stimulus Learning Values task containing:\n",
    "            'pushedChosen': 1 if participant pushed and 0 if participant pulled \n",
    "            'yellowChosen': 1 if participant chose yellow color and 0 if participant chose blue color\n",
    "            'winAmtPushable': the amount of feedback when participant pushed correctly, between  [0, 100]\n",
    "            'winAmtYellow  the amount of feedback when participant selected yellow color correcly, between  [0, 100]\n",
    "            'w': rewarded feedback coded to 1 and non-rewarded feedback coded to 0\n",
    "        \n",
    "    alpha_A : float [0, 1]\n",
    "        The learning rate related to Action Value Learning.\n",
    "      \n",
    "    alpha_C : float [0, 1]\n",
    "        The learning rate related to Color Value Learning.\n",
    "      \n",
    "    weight : float [0, 1]\n",
    "        The reelative contribution of Action and Stimulus Values Learning to get rewarded.\n",
    "\n",
    "    bet : float  [0, )\n",
    "        The sensitivity parameter in the soft_max choice rule.\n",
    "        the higher value leads to the more sensitivity to value differences between two options\n",
    "\n",
    "    init_probability : float in [0, 1] (default .5)\n",
    "        The initial value for the probability of reward.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "\n",
    "    data : pandas.DataFrame\n",
    "         Columns contains the task_frame, plus:\n",
    "        'alpha_A', 'alpha_C', 'weight', 'bet', 'w'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    task_fram = {'alpha_A':[],\n",
    "                 'alpha_C':[],\n",
    "                 'alpha_A':[],\n",
    "                 'weight':[],\n",
    "                 'bet':[],\n",
    "                 'alpha_A':[],\n",
    "                 'alpha_A':[],                \n",
    "                }\n",
    "    data = task_frame.copy()\n",
    " , \n",
    "    data['alpha_A'] = alpha_A\n",
    "    data['alpha_C'] = alpha_C\n",
    "    data['weight'] = weight\n",
    "    data['bet'] = bet\n",
    "    \n",
    "    for n in range(n_trials):\n",
    "        \n",
    "    data = pd.concat([data, _simulate_delta_rule_2A(task_design=task_design,\n",
    "                                                                   alpha=gen_alpha,\n",
    "                                                                   initial_value_learning=initial_value_learning)],\n",
    "                         axis=1)\n",
    "\n",
    "    elif type(gen_alpha) is list:\n",
    "        if len(gen_alpha) == 2:\n",
    "            data['alpha_pos'] = gen_alpha[0]\n",
    "            data['alpha_neg'] = gen_alpha[1]\n",
    "            data = pd.concat([data, _simulate_delta_rule_2A(task_design=task_design,\n",
    "                                                                       alpha=None,\n",
    "                                                                       initial_value_learning=initial_value_learning,\n",
    "                                                                       alpha_pos=gen_alpha[0],\n",
    "                                                                       alpha_neg=gen_alpha[1])],\n",
    "                             axis=1)\n",
    "\n",
    "        elif len(gen_alpha) == 3:\n",
    "            pass # implement here Stefano's learning rule\n",
    "        else:\n",
    "            raise ValueError(\"The gen_alpha list should be of either length 2 or 3.\")\n",
    "    else:\n",
    "        raise TypeError(\"The gen_alpha should be either a list or a float/int.\")\n",
    "\n",
    "    data['sensitivity'] = gen_sensitivity\n",
    "    data['p_cor'] = data.apply(_soft_max_2A, axis=1)\n",
    "    data['accuracy'] = stats.bernoulli.rvs(data['p_cor'].values) # simulate choices\n",
    "\n",
    "    data = data.set_index(['participant', 'block_label', 'trial_block'])\n",
    "    return data\n",
    "\n",
    "\n",
    "def delta_rule(task_design, alpha, initial_value_learning):\n",
    "    \"\"\"Q learning (delta learning rule) for two alternatives\n",
    "    (one correct, one incorrect).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    task_design : DataFrame\n",
    "        `pandas.DataFrame`, with n_trials_block*n_blocks rows.\n",
    "        Columns contain:\n",
    "        \"f_cor\", \"f_inc\", \"trial_type\", \"cor_option\", \"inc_option\",\n",
    "        \"trial_block\", \"block_label\", \"participant\".\n",
    "\n",
    "    alpha : float\n",
    "        The generating learning rate.\n",
    "        It should be a value between 0 (no updating) and 1 (full updating).\n",
    "\n",
    "    alpha_pos : float, default None\n",
    "        If a value for both alpha_pos and alpha_neg is provided,\n",
    "        separate learning rates are estimated\n",
    "        for positive and negative prediction errors.\n",
    "\n",
    "    alpha_neg : float, default None\n",
    "        If a value for both alpha_pos and alpha_neg is provided,\n",
    "        separate learning rates are estimated\n",
    "        for positive and negative prediction errors.\n",
    "\n",
    "    initial_value_learning : float\n",
    "        The initial value for Q learning.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Q_series : Series\n",
    "        The series of learned Q values (separately for correct and incorrect options).\n",
    "\n",
    "    \"\"\"\n",
    "    alpha = np.array([alpha])\n",
    "\n",
    "    n_trials = task_design.shape[0]\n",
    "\n",
    "    for n in range(n_trials):\n",
    "        index_cor = int(task_design.cor_option.values[n]-1)\n",
    "        Q = initial_value_learning\n",
    "        else:\n",
    "            if separate_learning_rates:\n",
    "                pe_cor = task_design.f_cor.values[n] - Q[index_cor]\n",
    "                pe_inc = task_design.f_inc.values[n] - Q[index_inc]\n",
    "                if pe_cor > 0:\n",
    "                    Q[index_cor] += alpha_pos[index_participant]*(task_design.f_cor.values[n] - Q[index_cor])\n",
    "                else:\n",
    "                    Q[index_cor] += alpha_neg[index_participant]*(task_design.f_cor.values[n] - Q[index_cor])\n",
    "                if pe_inc > 0:\n",
    "                    Q[index_inc] += alpha_pos[index_participant]*(task_design.f_inc.values[n] - Q[index_inc])\n",
    "                else:\n",
    "                    Q[index_inc] += alpha_neg[index_participant]*(task_design.f_inc.values[n] - Q[index_inc])\n",
    "            else:\n",
    "                Q[index_cor] += alpha[index_participant]*(task_design.f_cor.values[n] - Q[index_cor])\n",
    "                Q[index_inc] += alpha[index_participant]*(task_design.f_inc.values[n] - Q[index_inc])\n",
    "\n",
    "        Q_cor = np.append(Q_cor, Q[index_cor])\n",
    "        Q_inc = np.append(Q_inc, Q[index_inc])\n",
    "\n",
    "    return pd.DataFrame({'Q_cor':Q_cor, 'Q_inc':Q_inc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5435f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
