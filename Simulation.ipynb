{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5a9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add665fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44db1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Bernouli(theta = .5, n_samples = 1):\n",
    "    \"\"\"\n",
    "    Generating samples at random from Bernouli density funtion\n",
    "    \"\"\"\n",
    "    return (np.random.rand(n_samples) <= theta).astype(int)\n",
    "def shuffle_Binomial():\n",
    "    \"\"\"Shuffling bionomial number\"\"\"\n",
    "    data_bionom = np.concatenate([np.ones(21), np.zeros(21)]).astype(int)\n",
    "    np.random.shuffle(data_bionom)\n",
    "    \n",
    "    return data_bionom\n",
    "\n",
    "def shuffle_Amt(run = 1):\n",
    "    idx = np.linspace(1, 84, 84).astype(int)\n",
    "    amt = np.round(np.linspace(1,100,84)).astype(int)\n",
    "    if run == 1:\n",
    "        amt_output = amt[idx%2 == 1]\n",
    "        np.random.shuffle(amt_output)\n",
    "    elif run == 2:\n",
    "        amt_output = amt[idx%2 == 0]\n",
    "        np.random.shuffle(amt_output)\n",
    "    else:\n",
    "        amt_output = -1\n",
    "        \n",
    "    return amt_output\n",
    "\n",
    "def task_act_stim(isActFirst = True):\n",
    "    \"\"\"\n",
    "    This function performs the task desgin for Action an Stimulus Values learning\n",
    "    as Probabilistic Reiforcement Learning task to test the relative contribution of\n",
    "    Action and Rewarding mechanisms of Dopemiergic system in Parkinson disease.\n",
    "    The experimental task design was predefined and stored in .mat files. Also, it is the same for all participants.\n",
    "    There are two .m files, first is related to Action value learnig, when it is presented at the begining.\n",
    "                            another is related to Stimulus value leanring, when it is presented at the begining.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    firstAct: bool\n",
    "        True if the task starts with with Action-value, False if the task sarts with Stimulus-value learning\n",
    "        \n",
    "    Output\n",
    "    -------\n",
    "    data : pandas.DataFrame\n",
    "        Columns contains:'session',\n",
    "                         'run',\n",
    "                         'block',\n",
    "                         'trialNumber',\n",
    "                         'yellowOnLeftSide',\n",
    "                         'leftCanBePushed',\n",
    "                         'winAmtLeft',\n",
    "                         'winAmtRight',\n",
    "                         'winAmtYellow',the amount of feedback when participant selected yellow color correcly, between  [0, 100]\n",
    "                         'winAmtBlue',\n",
    "                         'winAmtPushable',\n",
    "                         'winAmtPullable',\n",
    "                         'pushCorrect': 1 if participant pushed and 0 if participant pulled \n",
    "                         'yellowCorrect':1 if participant chose yellow color and 0 if participant chose blue color\n",
    "    \"\"\"   \n",
    "\n",
    "    \n",
    "           \n",
    "    # The oputpu dataframe for task design generated by computer\n",
    "\n",
    "    # Two sessions, each session contains two runs\n",
    "    session = np.concatenate([np.repeat(1, 4*42), # session 1\n",
    "                              np.repeat(2, 4*42)]) # session 2\n",
    "\n",
    "    # Four runs, each run contains two blocks (conditions)\n",
    "    run = np.concatenate([np.repeat(1, 2*42), #The session 1 and run 1\n",
    "                          np.repeat(2, 2*42), #The session 1 and run 2\n",
    "                          np.repeat(1, 2*42), #The session 2 and run 1\n",
    "                          np.repeat(2, 2*42)]) #The session 2 and run 2\n",
    "\n",
    "    # the number of trials for eigh conditions, each condition (Action value or Stimulus Value) includes 42 trials\n",
    "    trialNumber = np.arange(1, 8*42 + 1)\n",
    "\n",
    "    # counterbalanced ledft and right stimulus response            \n",
    "    yellowOnLeftSide = np.concatenate([shuffle_Binomial(), shuffle_Binomial(),\n",
    "                                       shuffle_Binomial(), shuffle_Binomial(),\n",
    "                                       shuffle_Binomial(), shuffle_Binomial(),\n",
    "                                       shuffle_Binomial(), shuffle_Binomial()])   \n",
    "\n",
    "    # counterbalanced ledft and right action response\n",
    "    leftCanBePushed = np.concatenate([shuffle_Binomial(), shuffle_Binomial(), # Session 1 and Run 1\n",
    "                                      shuffle_Binomial(), shuffle_Binomial(), # Session 1 and Run 2\n",
    "                                      shuffle_Binomial(), shuffle_Binomial(), # Session 2 and Run 1\n",
    "                                      shuffle_Binomial(), shuffle_Binomial()]) # Session 2 and Run 2\n",
    "    # Alternative function sample_bernouli(theta = .5, n_samples = 42)\n",
    "\n",
    "\n",
    "    # winning reward for left side\n",
    "    winAmtLeft = np.concatenate([shuffle_Amt(run = 1), shuffle_Amt(run = 1), # Session 1 and Run 1\n",
    "                                 shuffle_Amt(run = 2), shuffle_Amt(run = 2), # Session 1 and Run 2\n",
    "                                 shuffle_Amt(run = 1), shuffle_Amt(run = 1), # Session 2 and Run 1\n",
    "                                 shuffle_Amt(run = 2), shuffle_Amt(run = 2)]) # Session 2 and Run 2\n",
    "    # winning reward for right side\n",
    "    winAmtRight = 100 - winAmtLeft  \n",
    "\n",
    "    # winning amounts for pulled respose\n",
    "    winAmtYellow = yellowOnLeftSide*winAmtLeft + (1 - yellowOnLeftSide)*winAmtRight\n",
    "    winAmtBlue = 100 - winAmtYellow       \n",
    "\n",
    "    # winning amounts for pushed respose\n",
    "    winAmtPushable = leftCanBePushed*winAmtLeft + (1 - leftCanBePushed)*winAmtRight\n",
    "    winAmtPullable = 100 - winAmtPushable\n",
    "\n",
    "            \n",
    "    # Announce choice correct for push and Yellow\n",
    "    pushCorrect = np.zeros(8*42).astype(int)\n",
    "    yellowCorrect = np.zeros(8*42).astype(int)\n",
    "    \n",
    "    for ses in range(2):\n",
    "        \"\"\"Two sessions applying altertative Action first and Stimlus first from predefined .m files\"\"\"\n",
    "        if isActFirst:\n",
    "            \"\"\"The action value learning is the first condition\"\"\"\n",
    "            data = loadmat('../data/ExpStruct_ActFirst_winOnly.mat')  \n",
    "            # If Action is first for the current participant\n",
    "            if ses==0:\n",
    "                # Each block is Action value or Stimulus Value condition\n",
    "                block = np.concatenate([np.repeat('Act', 42),  np.repeat('Stim', 42),\n",
    "                                        np.repeat('Stim', 42), np.repeat('Act', 42),\n",
    "                                        np.repeat('Stim', 42), np.repeat('Act', 42),\n",
    "                                        np.repeat('Act', 42), np.repeat('Stim', 42)])\n",
    "                stimActFirst = np.repeat('Act', 8*42)\n",
    "                \n",
    "            \n",
    "            # predefined pushed correct responce\n",
    "            pushCorrect[ses*4*42:(ses+1)*4*42] = np.concatenate([data['triallist1_1'][0], # The condition 1 and run 1\n",
    "                                                                shuffle_Binomial(), # The condition 1 and run 2\n",
    "                                                                  shuffle_Binomial(),  # The condition 2 and run 1\n",
    "                                                                  data['triallist2_2'][0]]) # The condition 2 and run 2\n",
    "            # predefined Yellow correct responce\n",
    "            yellowCorrect[ses*4*42:(ses+1)*4*42] = np.concatenate([shuffle_Binomial(), # The condition 1 and run 1\n",
    "                                                                data['triallist1_2'][0], # The condition 1 and run 2\n",
    "                                                                data['triallist2_1'][0],# The condition 2 and run 1\n",
    "                                                                shuffle_Binomial()]) # The condition 2 and run 2                    \n",
    "            isActFirst = False\n",
    "\n",
    "        else:\n",
    "            \"\"\"The stimulus value learning is the first condition\"\"\"\n",
    "            data = loadmat('../data/ExpStruct_StimFirst_winOnly.mat')\n",
    "            \n",
    "            # If Stimulus is first for the current participant\n",
    "            if ses==0:\n",
    "                # Each block is Action value or Stimulus Value condition\n",
    "                block = np.concatenate([np.repeat('Stim', 42),  np.repeat('Act', 42),\n",
    "                                        np.repeat('Act', 42), np.repeat('Stim', 42),\n",
    "                                        np.repeat('Act', 42), np.repeat('Stim', 42),\n",
    "                                        np.repeat('Stim', 42), np.repeat('Act', 42)])\n",
    "                stimActFirst = np.repeat('Stim', 8*42)\n",
    "                \n",
    "            pushCorrect[ses*4*42:(ses+1)*4*42] = np.concatenate([data['triallist1_1'][0], # The condition 1 and run 1\n",
    "                                                                shuffle_Binomial(), # The condition 1 and run 2\n",
    "                                                                  shuffle_Binomial(),  # The condition 2 and run 1\n",
    "                                                                  data['triallist2_2'][0]]) # The condition 2 and run 2\n",
    "            # predefined Yellow correct responce\n",
    "            yellowCorrect[ses*4*42:(ses+1)*4*42] = np.concatenate([shuffle_Binomial(), # The condition 1 and run 1\n",
    "                                                                data['triallist1_2'][0], # The condition 1 and run 2\n",
    "                                                                data['triallist2_1'][0],# The condition 2 and run 1\n",
    "                                                                shuffle_Binomial()]) # The condition 2 and run 2 \n",
    "            \n",
    "            isActFirst = True\n",
    "        \n",
    "        # Dictionary of task desing generated by computer\n",
    "        dataAct = ({'session':session,\n",
    "                    'run':run,\n",
    "                    'block':block,\n",
    "                     'stimActFirst':stimActFirst,\n",
    "                    'trialNumber':trialNumber,\n",
    "                    'yellowOnLeftSide':yellowOnLeftSide,\n",
    "                    'leftCanBePushed':leftCanBePushed,\n",
    "                    'winAmtLeft':winAmtLeft,\n",
    "                    'winAmtRight':winAmtRight,\n",
    "                    'winAmtYellow':winAmtYellow,\n",
    "                    'winAmtBlue':winAmtBlue,\n",
    "                    'winAmtPushable':winAmtPushable,\n",
    "                    'winAmtPullable':winAmtPullable,\n",
    "                    'pushCorrect':pushCorrect,\n",
    "                    'yellowCorrect':yellowCorrect})\n",
    "        # Dataframe of output\n",
    "        output = pd.DataFrame(dataAct)\n",
    "     \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2204b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_design = task_act_stim(isActFirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f92a31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47619047619047616"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_design[(task_design['session']==1)&(task_design['run']==1)&(task_design['block']=='Act')]['pushCorrect'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c955f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e6e642fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:64\u001b[0;36m\u001b[0m\n\u001b[0;31m    ,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def simulate_rl(task_act_stim, alpha_A, alpha_C, weight, beta, n_trilas = 10, init_probability=[.5, .5]):\n",
    "    \"\"\"\n",
    "    General Comment   \n",
    "    ----------\n",
    "    \n",
    "    Simulates a individual behavior for Action and Stimulus Value Learning \n",
    "    according to a RL model with the weightening parameter,\n",
    "\n",
    "    Notw that in this simulation, a simple Rescorla-Wagner rule is used for reinforcement learning\n",
    "    and the softmax function is used for the choice response\n",
    "\n",
    "    This function is to simulate data for, for example, parameter recovery.\n",
    "    Simulates data for one participant.\n",
    "    \n",
    "    Two rewarded feedback and non-rewarded feedback are presented in each trial.\n",
    "  \n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "\n",
    "    task_frame : pandas.DataFrame\n",
    "         Size of n_trials rows.\n",
    "            'w': rewarded feedback coded to 1 and non-rewarded feedback coded to 0\n",
    "        \n",
    "    alpha_A : float [0, 1]\n",
    "        The learning rate related to Action Value Learning.\n",
    "      \n",
    "    alpha_C : float [0, 1]\n",
    "        The learning rate related to Color Value Learning.\n",
    "      \n",
    "    weight : float [0, 1]\n",
    "        The reelative contribution of Action and Stimulus Values Learning to get rewarded.\n",
    "\n",
    "    beta : float  [0, )\n",
    "        The sensitivity parameter in the soft_max choice rule.\n",
    "        the higher value leads to the more sensitivity to value differences between two options\n",
    "\n",
    "    init_probability : float \n",
    "        The initial probability of reward for Run 1 and Run 2\n",
    "        The value should ne between 0 and 1 (default .5)\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "\n",
    "    data : pandas.DataFrame\n",
    "         Columns contains the task_frame, plus:\n",
    "        'alpha_A', 'alpha_C', 'weight', 'bet', 'w'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    task_fram = {'alpha_A':[],\n",
    "                 'alpha_C':[],\n",
    "                 'alpha_A':[],\n",
    "                 'weight':[],\n",
    "                 'bet':[],\n",
    "                 'alpha_A':[],\n",
    "                 'alpha_A':[],                \n",
    "                }\n",
    "    data = task_frame.copy()\n",
    " , \n",
    "    data['alpha_A'] = alpha_A\n",
    "    data['alpha_C'] = alpha_C\n",
    "    data['weight'] = weight\n",
    "    data['bet'] = bet\n",
    "    \n",
    "    for n in range(n_trials):\n",
    "        \n",
    "    data = pd.concat([data, _simulate_delta_rule_2A(task_design=task_design,\n",
    "                                                                   alpha=gen_alpha,\n",
    "                                                                   initial_value_learning=initial_value_learning)],\n",
    "                         axis=1)\n",
    "\n",
    "    elif type(gen_alpha) is list:\n",
    "        if len(gen_alpha) == 2:\n",
    "            data['alpha_pos'] = gen_alpha[0]\n",
    "            data['alpha_neg'] = gen_alpha[1]\n",
    "            data = pd.concat([data, _simulate_delta_rule_2A(task_design=task_design,\n",
    "                                                                       alpha=None,\n",
    "                                                                       initial_value_learning=initial_value_learning,\n",
    "                                                                       alpha_pos=gen_alpha[0],\n",
    "                                                                       alpha_neg=gen_alpha[1])],\n",
    "                             axis=1)\n",
    "\n",
    "        elif len(gen_alpha) == 3:\n",
    "            pass # implement here Stefano's learning rule\n",
    "        else:\n",
    "            raise ValueError(\"The gen_alpha list should be of either length 2 or 3.\")\n",
    "    else:\n",
    "        raise TypeError(\"The gen_alpha should be either a list or a float/int.\")\n",
    "\n",
    "    data['sensitivity'] = gen_sensitivity\n",
    "    data['p_cor'] = data.apply(_soft_max_2A, axis=1)\n",
    "    data['accuracy'] = stats.bernoulli.rvs(data['p_cor'].values) # simulate choices\n",
    "\n",
    "    data = data.set_index(['participant', 'block_label', 'trial_block'])\n",
    "    return data\n",
    "\n",
    "\n",
    "def delta_rule(task_design, alpha, initial_value_learning):\n",
    "    \"\"\"Q learning (delta learning rule) for two alternatives\n",
    "    (one correct, one incorrect).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    task_design : DataFrame\n",
    "        `pandas.DataFrame`, with n_trials_block*n_blocks rows.\n",
    "        Columns contain:\n",
    "        \"f_cor\", \"f_inc\", \"trial_type\", \"cor_option\", \"inc_option\",\n",
    "        \"trial_block\", \"block_label\", \"participant\".\n",
    "\n",
    "    alpha : float\n",
    "        The generating learning rate.\n",
    "        It should be a value between 0 (no updating) and 1 (full updating).\n",
    "\n",
    "    alpha_pos : float, default None\n",
    "        If a value for both alpha_pos and alpha_neg is provided,\n",
    "        separate learning rates are estimated\n",
    "        for positive and negative prediction errors.\n",
    "\n",
    "    alpha_neg : float, default None\n",
    "        If a value for both alpha_pos and alpha_neg is provided,\n",
    "        separate learning rates are estimated\n",
    "        for positive and negative prediction errors.\n",
    "\n",
    "    initial_value_learning : float\n",
    "        The initial value for Q learning.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Q_series : Series\n",
    "        The series of learned Q values (separately for correct and incorrect options).\n",
    "\n",
    "    \"\"\"\n",
    "    alpha = np.array([alpha])\n",
    "\n",
    "    n_trials = task_design.shape[0]\n",
    "\n",
    "    for n in range(n_trials):\n",
    "        index_cor = int(task_design.cor_option.values[n]-1)\n",
    "        Q = initial_value_learning\n",
    "        else:\n",
    "            if separate_learning_rates:\n",
    "                pe_cor = task_design.f_cor.values[n] - Q[index_cor]\n",
    "                pe_inc = task_design.f_inc.values[n] - Q[index_inc]\n",
    "                if pe_cor > 0:\n",
    "                    Q[index_cor] += alpha_pos[index_participant]*(task_design.f_cor.values[n] - Q[index_cor])\n",
    "                else:\n",
    "                    Q[index_cor] += alpha_neg[index_participant]*(task_design.f_cor.values[n] - Q[index_cor])\n",
    "                if pe_inc > 0:\n",
    "                    Q[index_inc] += alpha_pos[index_participant]*(task_design.f_inc.values[n] - Q[index_inc])\n",
    "                else:\n",
    "                    Q[index_inc] += alpha_neg[index_participant]*(task_design.f_inc.values[n] - Q[index_inc])\n",
    "            else:\n",
    "                Q[index_cor] += alpha[index_participant]*(task_design.f_cor.values[n] - Q[index_cor])\n",
    "                Q[index_inc] += alpha[index_participant]*(task_design.f_inc.values[n] - Q[index_inc])\n",
    "\n",
    "        Q_cor = np.append(Q_cor, Q[index_cor])\n",
    "        Q_inc = np.append(Q_inc, Q[index_inc])\n",
    "\n",
    "    return pd.DataFrame({'Q_cor':Q_cor, 'Q_inc':Q_inc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ff5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
